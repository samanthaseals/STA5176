---
title: "Multiple Regression: Diagnostics"
subtitle: "STA5176: Statistical Modeling"
author: "Dr. Seals"
format: 
  revealjs:
    theme: dark2
    self-contained: false
    slide-number: false
    footer: "[STA5176 - Statistical Modeling](https://samanthaseals.github.io/STA5176)"
    width: 1600
    height: 900
    chalkboard: true
    df-print: paged
    html-math-method: katex
editor: source
---

## Introduction

- In the last two lectures, we learned about linear regression,

$$ \hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + ... + \hat{\beta}_k x_k $$

- and the assumptions on the model,

$$\varepsilon \overset{\text{iid}}{\sim} N(0, \sigma^2)$$

- i.e., the <u>**residuals**</u> follow a normal distribution

    - Mean, $\mu=0$
    - Common variance, $\sigma^2$
    
## Introduction

- Today we will discuss regression diagnostics:

    - Outliers
    
    - Influential/leverage points
    
    - Collinearity
    
- Note that if we find an issue data point, we need to determine how it is affecting the results.

    - First instinct should **not** be to throw out an observation.
    
    - We will only remove a data point if we have a **non-statistical** reason to do so.
    
## Outliers

- Definition: data values that are much larger or smaller than the rest of the values in the dataset.

- We will look at the standardized residuals,

$$ e_{i_{\text{standardized}}} = \frac{e_i}{\sqrt{\text{MSE}(1-h_i)}}, $$

- where

    - $e_i = y_i - \hat{y}_i$ is the residual of the $i$^th^ observation
    - $h_i$ is the leverage of the $i$^th^ observation
    
- If $|e_{i_{\text{standardized}}}| > 2.5 \ \to \ $ outlier.

- If $|e_{i_{\text{standardized}}}| > 3 \ \to \ $ extreme outlier.

## Outliers: R Syntax

- We will use the `rstandard()` function to find the residuals.

- For ease of examining in large datasets, we will use it to create a "flag."

```{r, echo = TRUE, eval = FALSE}
[dataset] <- [dataset] %>%
  mutate(outlier = abs(rstandard([m]))>2.5)
```

- We can count the number of outliers,

```{r, echo = TRUE, eval = FALSE}
[dataset] %>% count(outlier)
```

- We can just look at outliers from the dataset,

```{r, echo = TRUE, eval = FALSE}
[new dataset] <- [dataset] %>% 
  filter(outlier == TRUE)
```

- We can also exclude outliers from the dataset,

```{r, echo = TRUE, eval = FALSE}
[new dataset] <- [dataset] %>% 
  filter(outlier == FALSE)
```

## Example

- Recall the penguin data and the model from last lecture,

```{r, echo = TRUE}
data <- palmerpenguins::penguins
m1 <- lm(body_mass_g ~ bill_length_mm + flipper_length_mm, data = data)
summary(m1)
```

$$ \hat{y} = -5536.90 + 6.05 \text{ bill} + 48.15 \text{ flipper} $$

## Outliers: Example

- Let's check for outlier penguins,

```{r, echo = TRUE, warning = FALSE, eval = FALSE}
library(tidyverse)
data <- data %>% mutate(outlier = abs(rstandard(m1))>2.5)
```

- Uh oh! This doesn't work!

<center><img src="images/L13a.png" width = 1000></center>

- R is telling us that we are trying to mutate a dataset with 344 observations using a model that was constructed on 342 observations.

    - Two penguins were excluded from the model due to missing data.

## Outliers: Example

- Let's try this again,

```{r, echo = TRUE, warning = FALSE}
library(tidyverse)
data2 <- palmerpenguins::penguins %>% 
  select(body_mass_g, bill_length_mm, flipper_length_mm) %>%
  na.omit() %>%
  mutate(obs = row_number())

m1 <- lm(body_mass_g ~ bill_length_mm + flipper_length_mm, data = data2)
summary(m1)
```

$$ \hat{y} = -5536.90 + 6.05 \text{ bill} + 48.15 \text{ flipper} $$

## Outliers: Example

- Let's again check for outlier penguins,

```{r, echo = TRUE}
data2 <- data2 %>% mutate(outlier = abs(rstandard(m1))>2.5)
head(data2)
```

## Outliers: Example

- How many penguins are outliers?

```{r, echo = TRUE}
data2 %>% count(outlier)
```

## Outliers: Example

- Let's look at the outliers,

```{r, echo = TRUE}
data3 <- data2 %>% filter(outlier == TRUE)
head(data3)
```

## Influence and Leverage

- Influence: A measure of how much the slope is affected by the observation.

- Leverage: A measure of the "extremeness" of an observation with respect to the independent variables.

- Cook's Distance ($d_i$): Measures the extent to which the estimates of the regression coefficients change when an observation is deleted from the analysis.

$$ d_i = \left( \frac{1}{k+1} \right) \left( \frac{h}{1-h_i} \right) e_{i_{\text{standardized}}} $$

- We will use the graphical assessment and look for "spikes."

## Influence and Leverage: R Syntax

- We will use the [`gg_cooksd()`](https://www.rdocumentation.org/packages/lindia/versions/0.9/topics/gg_cooksd) function from the [`lindia`](https://www.rdocumentation.org/packages/lindia/versions/0.9) package.

```{r, echo = TRUE, eval = FALSE}
gg_cooksd([m]) 
```

- Note that this is built in `ggplot()` -- you can add on as you wish

## Influence and Leverage: Example

- Let's find the Cook's distance graph for the penguin model.

```{r, echo = TRUE}
library(lindia)
gg_cooksd(m1) + theme_bw()
```

- I would investigate points 39, 169, 292, and 313.

## Multicollinearity

- Collinearity/multicollinearity: a correlation between two or more predictor variables affects the estimation procedure.

- We will use the variance inflation factor (VIF) to check for multicollinearity.

$$ \text{VIF}_j = \frac{1}{1-R^2_j}, $$

- where

    - $j$ = the predictor of interest and $j \in \{1, 2, ..., k \}$,
    - $R^2_j$ results from regressing $X_j$ on the remaining $(k-1)$ predictors.
    
- We say that multicollinearity is present if VIF > 10.

## Multicollinearity

- How do we deal with multicollinearity?

    - Easy answer: remove at least one predictor from the collinear set, then reassess VIF.
    
    - More complicated: how do we know which predictor should be the one removed?
    
        - (We will likely need to consult with the research team.)
        
- Notes for more advanced models:

    - If categorical predictors are present in the model, multicollinearity may be difficult to assess.
    
    - We <u>must not</u> include interaction terms when assessing VIF.
    
## Multicollinearity: R Syntax

- We will use the [`vif()`](https://www.rdocumentation.org/packages/car/versions/3.1-0/topics/vif) function from the `car` package.

```{r, echo = TRUE, eval = FALSE}
vif([m])
```

- There will be a value for each predictor in the model.
    
## Multicollinearity: Example

- Let's check the multicollinearity for our penguin model,

```{r, echo = TRUE, warning = FALSE}
library(car)
vif(m1)
```

- No multicollinearity is present.

## Multicollinearity: Example

```{r, echo = TRUE}
m2 <- lm(body_mass_g ~ bill_length_mm + flipper_length_mm + bill_length_mm:flipper_length_mm, data = data)
vif(m2)
```

## Sensitivity Analysis

- We can remove problem datapoints and reanalyze the data to see "how different" the results are. 

- Let's remove the problem datapoints from the penguin dataset and reanalyze.

```{r, echo = TRUE}
data4 <- data2 %>%
  filter(outlier == FALSE & !obs %in% c(39, 169, 292, 313)) 
m3 <- lm(body_mass_g ~ bill_length_mm + flipper_length_mm, data = data4)
summary(m3)
```

$$ \hat{y} = -5752.56 + 5.48 \text{ bill} + 48.29 \text{ flipper} $$

## Sensitivity Analysis

$$ \hat{y} = -5536.90 + 6.05 \text{ bill} + 48.15 \text{ flipper} $$

- Bill length is not significant (*p* = 0.244).
- Flipper length is significant (*p* < 0.001).


$$ \hat{y} = -5752.56 + 5.48 \text{ bill} + 48.29 \text{ flipper} $$

- Bill length is not significant (*p* = 0.281).
- Flipper length is significant (*p* < 0.001).

## Don't Delete this Slide

More playing with GitHub branches


