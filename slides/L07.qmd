---
title: "Linear Regression and Correlation"
subtitle: "STA5176 Lecture 7, Summer 2023"
execute:
  echo: true
  warning: false
  message: false
format: 
  revealjs:
    theme: uwf
    self-contained: true
    slide-number: false
    footer: "[STA5176 - Statistical Modeling](https://samanthaseals.github.io/STA5176)"
    width: 1600
    height: 900
    df-print: paged
    html-math-method: katex
editor: source
---

## Introduction

- Previously discussed analyzing continuous data

    - Two-sample *t*-test
    - One-way ANOVA
    - Two-way ANOVA
    
- Commonality: comparing groups

- New: thinking about this in model form; continuous predictor

## Introduction

- **Response variable (outcome)**: the dependent variable; what we are trying to model the outcome of; *y*

- **Explanatory variable (predictor)**: the independent variable; what we are using to model the outcome; *x*

- Nomenclature: 

    - -variate: describes number of outcomes
    - -variable: describes number of predictors

- Special case of regression: simple linear regression

    - One continuous outcome
    - One continuous predictor
    
## Prediction Equation: Population

- The prediction equation for the **population**: $$ y = \beta_0 + \beta_1 x $$

- $\beta_0$ and $\beta_1$ are **parameters**

    - $\beta_0$ is the $y$-intercept
    - $\beta_1$ is the slope

- $y$ is the observed outcome
    
## Prediction Equation: Sample

- The prediction equation for the **sample**: $$ \hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x $$

- $\hat{\beta}_0$ and $\hat{\beta}_1$ are **statistics**

    - $\hat{\beta}_0$ is the estimate of the $y$-intercept
    - $\hat{\beta}_1$ is the estimate of the slope

- $y$ is the predicted outcome

## Prediction Equation: Graphically

<center><img src="images/L11a.png" width = 1200></center><br>

- "line of best fit"

## Linear Regression: Introduction

- We are using data to quantitatively explain the relationship between $x$ and $y$.

    - As $x$ increases, what happens to $y$?
    
- Linear regression assumes linearity. 

    - $\beta_1$ is constaint: as $x$ increases, the slope does not change.
    
## Linear Regression: Introduction

- Note that $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$ is an exact function.

    - There is no room for error!

<center><img src="images/L11b.png" width = 1000></center>

## Linear Regression: Introduction

- Thus, we will add an error term:

\begin{align*}
  \text{Population: } & y = \beta_0 + \beta_1 x + \varepsilon \\
  \text{Sample: } & y = \hat{\beta}_0 + \hat{\beta}_1 x + e
\end{align*}

- where $\varepsilon$ and $e$ are the error terms that measure the difference between the expected/predicted value and the observed value.

- Note that:

    - ($\beta_0 + \beta_1 x$) and ($\hat{\beta}_0 + \hat{\beta}_1 x$) are the predictable parts of the model,
    - $\varepsilon$ and $e$ are the unpredictable parts of the models,
    - and $\varepsilon \overset{\text{iid}}{\sim} N(0, \sigma^2)$

## Linear Regression: Assumptions

- Remember the ANOVA assumptions? (ANOVA = regression!)

$$\varepsilon \overset{\text{iid}}{\sim} N(0, \sigma^2)$$

- The <u>**residuals**</u> follow a normal distribution

    - Mean, $\mu=0$
    - Common variance, $\sigma^2$

- In R, we assess the assumptions as we did in ANOVA.

## Linear Regression: Example

- Consider a basic example: the cost of road resurfacing. 

    - *y* is the cost in thousands of dollars,
    - *x* is the mileage
    
<center><img src="images/L11c.png" width = 800></center>

## Linear Regression: Computation

- We will estimate the population regression line using the sample regression line,

$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$$

- We will use ordinary least squares to estimate regression coefficients.

$$ \hat{\beta}_1 = \frac{S_{xy}}{S_{xx}} \ \ \ \text{ and} \ \ \ \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x} $$

- where

$$ S_{xx} = \sum_{i=1}^n x_i^2 - \frac{(\sum_{i=1}^n x_i)^2}{n} \ \ \ \text{ and} \ \ \ S_{xy} = \sum_{i=1}^n x_i y_i - \frac{\sum_{i=1}^n x_i \sum_{i=1}^n y_i}{n} $$

## Linear Regression: R Syntax

- We used the `lm()` function in ANOVA -- we will use it again for linear regression (ANOVA = regression!)

```{r, echo = TRUE, eval = FALSE}
m <- lm([outcome] ~ [predictor],
        data = [dataset])
```

- To see the regression information, we will run the result through the `summary()` function.

```{r, echo = TRUE, eval = FALSE}
summary(m)
```

## Linear Regression: Example

- Let's find the regression line for the road resurfacing data. 

```{r}
library(tidyverse)
x <- c(1, 3, 4, 5, 7)
y <- c(6, 14, 10, 14, 26)
data <- tibble(x, y)
```

```{r, echo = TRUE}
m1 <- lm(y ~ x, data = data)
summary(m1)
```

$$ \hat{y} = 2 + 3x $$

## Linear Regression: Example

<center><img src="images/L11d.png" width = 1200></center>

## Linear Regression: Interpretations

- We must interpret the regression coefficients. 

    - Intercept: when [predictor = 0], the average [outcome] is [$\hat{\beta}_0$].

    - Slope: For a 1 [units of predictor] increase in [predictor], we expect [outcome] to [increase or decrease] by [$|\hat{\beta}_1|$] [units of outcome].
    
        - If $\hat{\beta}_1 > 0$, there is an increase.
        - If $\hat{\beta}_1 < 0$, there is a decrease.
        
## Linear Regression: Example

- From the road surfacing example,

$$ \hat{y} = 2 + 3x $$

- Intercept: before considering mileage, we expect road resurfacing will cost $2,000.

- Slope: for every mile resurfaced, we expect the cost to increase by $3,000.

- Slope: for every 10 miles resurfaced, we expect the cost to increase by $30,000.

## Linear Regression: Testing Slope

- **Hypotheses**

  - $H_0: \beta_1 = \beta_1^{(0)}$
  - $H_1: \beta_1 \ne \beta_1^{(0)}$


- **Test Statistic**

    - $$ t_0 = \frac{\hat{\beta}_1 - \beta_1^{(0)}}{\text{SE}_{\hat{\beta}_1}} $$ where

        -   $\beta_1^{(0)}$ is the hypothesized value (often 0)
        -   $\text{SE}_{\hat{\beta}_1} = \sqrt{\text{MSE}/S_{xx}}$
    
- ***p*-Value**

    - $p = 2 \times P[t_{1, 1 - \alpha/2} \ge t_0]$
    
## Linear Regression: Example

- Recall the road resurfacing example.

```{r, echo = TRUE}
summary(m1)
```

## Linear Regression: Example

- **Hypotheses**

    - $H_0: \beta_1 = 0$
    - $H_1: \beta_1 \ne 0$

- **Test Statistic and *p*-value**

    - $t_0 = 3.503$

    - $p = 0.0394$
    
- **Rejection Region**

    - Reject $H_0$ if $p < \alpha$; $\alpha = 0.05$

- **Conclusion/Interpretation**

    - Reject $H_0$. There is sufficient evidence to suggest that mileage is a significant predictor of cost.

## Linear Regression: Confidence Interval for Slope

**Confidence Interval for $\beta_1$:**

$$\hat{\beta}_1 \pm t_{\alpha/2, n-2} \text{SE}_{\hat{\beta}_1}$$

- We will use the `confint()` function to find the confidence interval.

```{r, echo = TRUE}
confint(m1)
```

- The 95% CI for $\beta_1$ is (0.27, 5.73)

```{r, echo = TRUE}
confint(m1, level = 0.99)
```

- The 99% CI for $\beta_1$ is (-2.00, 8.00)

## Correlation: Pearson's Introduction

- Unfortunately, $\beta_1$ does not tell us the *strength* of the relationship between *x* and *y*.

    - This is because of the units involved both on *x* and *y*.

- We will now look at a unitless measurement, called correlation.

    - Correlation: a unitless measurement of the strength of the **linear** relationship between two variables.
    
- The stronger the relationship, the better *x* predicts *y* in $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1x$.

## Correlation: Pearson's Introduction

- The population correlation is $\rho$ ("rho").

- The sample correlation is $r$.

- Correlation $\in$ [-1, 1]

    - The closer to +/- 1, the stronger the relationship.
    - $\rho>0$: positive correlation; as *x* increases, *y* increases
    - $\rho<0$: negative correlation; as *x* increases, *y* decreases
    - $\rho=0$: no correlation; as *x* increases, *y* is constant

## Correlation: Pearson's Computation

- Given $n$ pairs of observations $(x_i, y_i)$, Pearson's correlation is as follows:

$$ r = \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}, $$

- where:

$$ S_{xx} = \sum_{i=1}^n x_i^2 - \frac{(\sum_{i=1}^n x_i)^2}{n} \ \ \ \text{ and} \ \ \ S_{yy} = \sum_{i=1}^n y_i^2 - \frac{(\sum_{i=1}^n y_i)^2}{n}  $$

$$S_{xy} = \sum_{i=1}^n x_i y_i - \frac{\sum_{i=1}^n x_i \sum_{i=1}^n y_i}{n} $$

## Correlation: Pearson's R Syntax

- We will use the `cor()` function to find the correlation.

```{r, echo = TRUE, eval = FALSE}
cor([dataset]$[var1], [dataset]$[var2], 
    use = "complete.obs") # single correlation

cor(dataset, 
    use = "complete.obs") # correlation matrix
```

- While we can find the correlation matrix for 

- Note that there are several other packages/functions out there to help with visualization of correlation!

## Correlation: Example

- Let's revisit the penguins dataset and find the correlation matrix.

```{r, echo = TRUE}
data <- palmerpenguins::penguins %>%
  select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)
cor(data, use = "complete.obs") 
```

- Let's look at just the correlation between bill length and flipper length.

```{r, echo = TRUE}
cor(data$bill_length_mm, data$flipper_length_mm, use = "complete.obs")
```


## Correlation: Confidence Interval

**Confidence Interval for $\rho$**

$$ \left(\frac{e^{2z_1}-1}{e^{2z_1}+1}, \frac{e^{2z_2}-1}{e^{2z_2}+1} \right), $$

- where:

$$ z_1 = z- \frac{z_{\alpha/2}}{\sqrt{n-3}} \ \ \ \text{ and} \ \ \ z_2 = z+ \frac{z_{\alpha/2}}{\sqrt{n-3}} $$

- and: 

$$ z = \frac{1}{2} \ln\left(\frac{1+r}{1-r} \right)$$

## Correlation: R Syntax

- We will now use the `cor.test()` function.

```{r, echo = TRUE, eval = FALSE}
cor.test([dataset]$[var1], [dataset]$[var2], 
         conf.level = [level],
         use = "complete.obs")
```

## Correlation: Example

```{r, echo = TRUE}
cor.test(data$bill_length_mm, data$flipper_length_mm, use = "complete.obs")
```

- The 95% CI for $\rho$ is (0.59, 0.71).

```{r, echo = TRUE}
cor.test(data$bill_length_mm, data$flipper_length_mm, conf.level = 0.99, use = "complete.obs")
```

- The 99% CI for $\rho$ is (0.57, 0.73).

## Correlation: Assumptions

- Note that the assumption on Pearson's correlation is that **both** variables are normally distributed.

    - But... in regression, I don't "care" about the distribution of *x*!
    
    - And... we also don't "care" about the distribution of *y*.
    
    - (We only care about the distribution of the residuals.)
    
- Thus, need to check assumption of normality.

    - I use histograms for quick and dirty check. 
    
    - We could use q-q plots...
    
## Correlation: Example

```{r, echo = FALSE}
library(ggpubr)
p1 <- data %>% ggplot(aes(sample = bill_length_mm)) +
  stat_qq(size=3) +
  stat_qq_line() +
  theme_minimal() +
  xlab("Theoretical") +
  ylab("Sample") +
  ggtitle("Bill Length") +
  theme(text = element_text(size=20))
p2 <- data %>% ggplot(aes(sample = flipper_length_mm)) +
  stat_qq(size=3) +
  stat_qq_line() +
  theme_minimal() +
  xlab("Theoretical") +
  ylab("Sample") +
  ggtitle("Flipper Length") +
  theme(text = element_text(size=20))
ggarrange(p1, p2, nrow = 1)
```


## Correlation: Spearman's Introduction

- If we do not meet the assumption for Pearson's correlation, we can use Spearman's correlation.

    - This is a nonparametric method to finding the corerlation coefficient.
    
- Algorithm:

    - 1. Sort by *x*, assign ranks (*R<sub>x</sub>*)
    - 2. Sort by *y*, assign ranks (*R<sub>y</sub>*)
    - 3. Compute Pearson's on (*R<sub>x</sub>*, *R<sub>y</sub>*)
    
- Note that we are transforming (*x*, *y*) $\to$ (*R<sub>x</sub>*, *R<sub>y</sub>*)

## Correlation: Spearman's R Syntax

- We again use the `cor.test()` function, but specify Spearman's.

```{r, echo = TRUE, eval = FALSE}
cor([dataset]$[var1], [dataset]$[var2], 
    use = "complete.obs", 
    method = "spearman") # single correlation

cor(dataset, 
    use = "complete.obs", 
    method = "spearman") # correlation matrix
```


## Correlation: Example

- Let's revisit the penguins dataset and find the correlation matrix.

```{r, echo = TRUE}
cor(data, use = "complete.obs", method = "spearman") 
```

- Let's look at just the correlation between bill length and flipper length.

```{r, echo = TRUE}
cor(data$bill_length_mm, data$flipper_length_mm, use = "complete.obs", method = "spearman")
```

## Model Visualization

- While we are in simple linear regression, visualization is easy. 

- We will first plot a scatterplot, then overlay the regression line.

    - The predictor will be on the *x*-axis.
    - The outcome will be on the *y* axis.
    - We will create predicted values to construct the line.
    
## Model Visualization: Example

- Let's create a model using the penguin data,

```{r, echo = TRUE}
m2 <- lm(flipper_length_mm ~ bill_length_mm, data = data)
(c2 <- coefficients(m2))
```

- Let's create the predicted values,

```{r, echo = TRUE}
data <- data %>%
  mutate(yhat = c2[1] + c2[2]*bill_length_mm)
head(data, n = 2)
```

## Model Visualization: Example

- Now, let's build our `ggplot()`

::: {.panel-tabset}

## Code
```{r, echo = TRUE, eval = FALSE}
data %>% ggplot(aes(x = bill_length_mm, y = flipper_length_mm)) +
  geom_point(size=4, color = "darkgray") + 
  geom_line(aes(y = yhat)) +
  labs(x = "Bill Length (mm)",
       y = "Flipper Length (mm)") +
  theme_bw()
```

## Graph

<center>
```{r, echo = FALSE, eval = TRUE}
data %>% ggplot(aes(x = bill_length_mm, y = flipper_length_mm)) +
  geom_point(size=4, color = "darkgray") + 
  geom_line(aes(y = yhat)) +
  labs(x = "Bill Length (mm)",
       y = "Flipper Length (mm)") +
  theme_bw()
```
</center>

:::

## Multiple Regression: Introduction

- We have learned simple linear regression,

$$ \hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x $$

- Now, let's expand to multiple regression,

$$ \hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1  + \hat{\beta}_2 x_2 + ... + \hat{\beta}_k x_k$$

## Multiple Regression: Computation

- In simple linear regression, there are formulas for estimation of $\beta_0$ and $\beta_1$

- However, the true underlying computation is matrix multiplication: $$ y = X\beta + \varepsilon, $$ where:

    - $y$ is the vector of outcome responses,
    - $X$ is the design matrix
    
        - first column is all 1's
        - following columns are vectors of predictor responses
  
    - $\varepsilon$ is the vector of error terms
    
## Multiple Regression: R Syntax

- We again use the `lm()` function to construct our model.

```{r, echo = TRUE, eval = FALSE}
m <- lm([outcome] ~ [pred_1] + [pred_2] + ... + [pred_k], data = data)
```

- To see the full summary of the model, we will use the `summary()` function.
    
```{r, echo = TRUE, eval = FALSE}
summary(m)
``` 

- To find the confidence intervals for $\beta_i$, we will use the `confint()` function.

```{r, echo = TRUE, eval = FALSE}
confint(m)
```
    
## Multiple Regression: Example

- Recall the penguin data. Let's model body mass (g) as a function of bill length (mm) and flipper length (mm).

```{r, echo = TRUE}
data <- palmerpenguins::penguins
m1 <- lm(body_mass_g ~ bill_length_mm + flipper_length_mm, data = data)
summary(m1)
```

$$ \hat{y} = -5536.90 + 6.05 \text{ bill} + 48.15 \text{ flipper} $$
    
## Multiple Regression: Interpretations

- We interpret coefficients much the same as in simple linear regression.

    - Intercept: when [all predictors = 0], the average [outcome] is [$\hat{\beta}_0$].

    - Slope: For a 1 [units of predictor i] increase in [predictor i], we expect [outcome] to [increase or decrease] by [$|\hat{\beta}_i|$] [units of outcome], after controlling for [all other predictors in the model.
    
        - If $\hat{\beta}_i > 0$, there is an increase.
        - If $\hat{\beta}_i < 0$, there is a decrease.
    
## Multiple Regression: Example

- Let's interpret the penguin model.

```{r, echo = TRUE}
coefficients(m1)
```

- When both bill length and flipper length are 0 mm, a penguin's body mass is -5736.9 g.
- As bill length increases by 1 mm, the penguin's body mass increases by 6 g, after controlling for flipper length.
- As flipper length increases by 1 mm, the penguin's body mass increases by 48 g, after controlling for bill length.

## Multiple Regression: Inference on Slope

- **Hypotheses**

    - $H_0: \beta_i = \beta_i^{(0)}$
    - $H_1: \beta_i \ne \beta_i^{(0)}$

- **Test Statistic**

    - $$ t_0 = \frac{\hat{\beta}_i - \beta_i^{(0)}}{\text{SE}_{\hat{\beta}_i}}, $$ where

        -   $\beta_i^{(0)}$ is the hypothesized value (often 0)
        -   SE$_{\hat{\beta}_i}$ = the $i$^th^ diagonal entry of  $\sqrt{\text{MSE}(X^{'}X)^{-1}}$ 
        
- ***p*-Value**        

    - $p = 2 \times P[t_{1, 1 - \alpha/2} \ge t_0]$
    
- **Rejection Region**

    - Reject $H_0$ if $p < \alpha$
    
## Multiple Regression: R Syntax

- To see the *t*-test output, we will use the `summary()` function.
    
```{r, echo = TRUE, eval = FALSE}
summary(m)
``` 

- To find the confidence intervals for $\beta_i$, we will use the `confint()` function.

```{r, echo = TRUE, eval = FALSE}
confint(m)
```
    
## Multiple Regression: Example

```{r, echo = TRUE}
summary(m1)
```
    
## Multiple Regression: Example

- **Hypotheses**

    - $H_0: \beta_{\text{Bill}} = 0$
    - $H_1: \beta_{\text{Bill}} \ne 0$

- **Test Statistic and *p*-value**

    - $t_0 = 1.168$
    - $p = 0.244$
    
- **Rejection Region**

    - Reject $H_0$ if $p < \alpha$; $\alpha = 0.05$

- **Conclusion/Interpretation**

    - Fail to reject $H_0$. After adjusting for flipper length, there is not sufficient evidence to suggest that bill length is a significant predictor of penguin body mass.


## Multiple Regression: Example

- **Hypotheses**

    - $H_0: \beta_{\text{Flipper}} = 0$
    - $H_1: \beta_{\text{Flipper}} \ne 0$

- **Test Statistic and *p*-value**

    - $t_0 = 23.94$
    - $p < 0.001$
    
- **Rejection Region**

    - Reject $H_0$ if $p < \alpha$; $\alpha = 0.05$

- **Conclusion/Interpretation**

    - Reject $H_0$. After adjusting for bill length, there is sufficient evidence to suggest that flipper length is a significant predictor of penguin body mass.

## Multiple Regression: Example

```{r, echo = TRUE}
confint(m1)
```

- The 95% CI for $\beta_{\text{Bill}}$ is (-4.14, 16.24).

- The 95% CI for $\beta_{\text{Flipper}}$ is (44.19, 52.10).

## Multiple Regression: Inference on the Line

- ANOVA allows us to test for the significance of the regression line.

    - Partitioning the variance into variance due to the model and the error term.
    
    - The error term holds the variance due to variables *not* included in the predictor set.
    
- We will not concern ourselves with the computation of sums of squares.

    - Mean squares are computed like before: SS<sub>X</sub> / df<sub>X</sub>
    
    - Test statistic is *F* = MS<sub>Reg</sub> / MS<sub>E</sub>
    
    - The degrees of freedom are as follows:
    
        - df<sub>Reg</sub> = *k*
        - df<sub>E</sub> = *n*--*k*--1
        - df<sub>Tot</sub> = *n*--1
        
## Multiple Regression: Inference on the Line

- **Hypotheses**

    - $H_0: \beta_1 = \beta_2 = ... = \beta_k = 0$
    - $H_1:$ at least one $\beta_i$ is different.

- **Test Statistic**

    - $F_0 = \text{MS}_{\text{Reg}} / \text{MS}_{\text{E}}$ 

- ***p*-Value**

    - $p = P[F_{\text{df}_{\text{Reg}}, \text{df}_{\text{E}}, 1-\alpha/2} \ge F_0]$

- **Rejection Region**

    - Reject $H_0$ if $p < \alpha$; $\alpha = 0.05$

## Multiple Regression: R Syntax

- The ANOVA information comes out of the `summary()` function

```{r, echo = TRUE}
summary(m1)
```

## Multiple Regression: Example

- **Hypotheses**

    - $H_0: \beta_1 = \beta_2 = 0$
    - $H_1:$ at least one $\beta_i$ is different.

- **Test Statistic and *p*-Value**

    - $F_0 =  536.6$
    - $p < 0.001$
    
- **Rejection Region**

    - Reject $H_0$ if $p < \alpha$; $\alpha = 0.05$

- **Conclusion/Interpretation**

    - Reject $H_0$. There is sufficient evidence to suggest that at least one variable is a significant predictor of penguin body mass.

## Multiple Regression: Line Fit

- We can assess how well the regression model fits the data using $R^2$.

$$ R^2 = \frac{\text{SS}_{\text{Reg}}}{\text{SS}_{\text{Tot}}} $$

- Thus, $R^2$ is the proportion of variation explained by the model (i.e., predictor set).

- $R^2 \in [0, 1]$

    - $R^2 \to 0$ indicates that the model fits "poorly."
    
    - $R^2 \to 1$ indicates that the model fits "well."

    - $R^2 = 1$ indicates that all points fall on the response surface.
    


## Multiple Regression: Line Fit

- Recall that the error term is the "catch all" ...

    - The SS<sub>Tot</sub> is constant for the outcome of interest.
    - As we add predictors to the model, we are necessarily increasing SS<sub>Reg</sub>
    
        - The variance is moving from SS<sub>E</sub> to SS<sub>Reg</sub>
        
- We do not want to arbitrarily increase $R^2$, so we will use an adjusted version:

$$ R^2_{\text{adj}} = 1 - \frac{\text{MS}_{\text{E}}}{\text{SS}_{\text{Tot}}/\text{df}_{\text{Tot}}} $$

## Multiple Regression: R Syntax

- The $R^2$ and $R^2_{\text{adj}}$ both come out of the `summary()` function.

```{r, echo = TRUE}
summary(m1)
```

- $R^2_{\text{adj}}$ is 0.759 -- 75.9% of the variability in body mass is explained by the model with bill length and flipper length.

## Model Visualization: Introduction

- Like before, we can visualize our model. 

    - It is now more complicated because we have more than one predictor in the model.
    
- We will first plot a scatterplot, then overlay the regression line.

    - One predictor will be on the *x*-axis.
    - The outcome will be on the *y*-axis.
    - We can plot one or more lines by plugging in for the predictors not on the *x*-axis.
    
## Model Visualization: Example

- Let's create a model using the penguin data,

```{r, echo = TRUE}
(c1 <- coefficients(m1))
```    
    
- Let's have flipper length on the *x*-axis. This means we will plug in for bill length,

```{r, echo = TRUE}
quantile(data$bill_length_mm, na.rm = TRUE)
```

- Let's create lines for bill lengths of 35, 45, and 55 mm.

```{r, echo = TRUE}
library(tidyverse)
data <- data %>%
  mutate(b35 = c1[1] + c1[2]*35 + c1[3]*flipper_length_mm,
         b45 = c1[1] + c1[2]*45 + c1[3]*flipper_length_mm,
         b55 = c1[1] + c1[2]*55 + c1[3]*flipper_length_mm)
```    

## Model Visualization: Example

```{r}
head(data, n = 5)
```

## Model Visualization: Example

- Now we can construct the graph using ``ggplot()``:

::: {.panel-tabset}

## Code
```{r, echo = TRUE, eval = FALSE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "pink") +
  geom_line(aes(y = b45), color = "purple") +
  geom_line(aes(y = b55), color = "blue") +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```

## Graph
<center>
```{r, echo = FALSE, eval = TRUE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "pink") +
  geom_line(aes(y = b45), color = "purple") +
  geom_line(aes(y = b55), color = "blue") +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```
</center>

:::

## Model Visualization: Example

- We can further tweak the graph:

::: {.panel-tabset}

## Previous Code
```{r, echo = TRUE, eval = FALSE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "pink") +
  geom_line(aes(y = b45), color = "purple") +
  geom_line(aes(y = b55), color = "blue") +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```

## Previous Graph

<center>
```{r, echo = FALSE, eval = TRUE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "pink") +
  geom_line(aes(y = b45), color = "purple") +
  geom_line(aes(y = b55), color = "blue") +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```
</center>

## Updated Code

```{r, echo = TRUE, eval = FALSE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "pink") +
  geom_line(aes(y = b45), color = "purple") +
  geom_line(aes(y = b55), color = "blue") +
  geom_text(aes(x = 234, y = 5600, label = "35 mm")) +
  geom_text(aes(x = 234, y = 5700, label = "45 mm")) + 
  geom_text(aes(x = 234, y = 5800, label = "55 mm")) +
  geom_text(aes(x = 234, y = 5950, label = "Bill Length")) +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```

## Updated Graph

<center>
```{r, echo = FALSE, eval = TRUE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "pink") +
  geom_line(aes(y = b45), color = "purple") +
  geom_line(aes(y = b55), color = "blue") +
  geom_text(aes(x = 234, y = 5600, label = "35 mm")) +
  geom_text(aes(x = 234, y = 5700, label = "45 mm")) + 
  geom_text(aes(x = 234, y = 5800, label = "55 mm")) +
  geom_text(aes(x = 234, y = 5950, label = "Bill Length")) +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```
</center>

:::

## Model Visualization: Example

- We can further tweak the graph:

::: {.panel-tabset}

## Previous Code

```{r, echo = TRUE, eval = FALSE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "pink") +
  geom_line(aes(y = b45), color = "purple") +
  geom_line(aes(y = b55), color = "blue") +
  geom_text(aes(x = 234, y = 5600, label = "35 mm")) +
  geom_text(aes(x = 234, y = 5700, label = "45 mm")) + 
  geom_text(aes(x = 234, y = 5800, label = "55 mm")) +
  geom_text(aes(x = 234, y = 5950, label = "Bill Length")) +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```

## Previous Graph

<center>
```{r, echo = FALSE, eval = TRUE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "pink") +
  geom_line(aes(y = b45), color = "purple") +
  geom_line(aes(y = b55), color = "blue") +
  geom_text(aes(x = 234, y = 5600, label = "35 mm")) +
  geom_text(aes(x = 234, y = 5700, label = "45 mm")) + 
  geom_text(aes(x = 234, y = 5800, label = "55 mm")) +
  geom_text(aes(x = 234, y = 5950, label = "Bill Length")) +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```
</center>

## Updated Code

```{r, echo = TRUE, eval = FALSE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "black") +
  geom_line(aes(y = b45), color = "black") +
  geom_line(aes(y = b55), color = "black") +
  geom_text(aes(x = 234, y = 5600, label = "35 mm")) +
  geom_text(aes(x = 234, y = 5700, label = "45 mm")) + 
  geom_text(aes(x = 234, y = 5800, label = "55 mm")) +
  geom_text(aes(x = 234, y = 5950, label = "Bill Length")) +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```

## Updated Graph

<center>
```{r, echo = FALSE, eval = TRUE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "black") +
  geom_line(aes(y = b45), color = "black") +
  geom_line(aes(y = b55), color = "black") +
  geom_text(aes(x = 234, y = 5600, label = "35 mm")) +
  geom_text(aes(x = 234, y = 5700, label = "45 mm")) + 
  geom_text(aes(x = 234, y = 5800, label = "55 mm")) +
  geom_text(aes(x = 234, y = 5950, label = "Bill Length")) +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```
</center>

## Regression Diagnostics: Introduction

- Now let's discuss regression diagnostics:

    - Outliers
    
    - Influential/leverage points
    
    - Collinearity
    
- Note that if we find an issue data point, we need to determine how it is affecting the results.

    - First instinct should **not** be to throw out an observation.
    
    - We will only remove a data point if we have a **non-statistical** reason to do so.
    
## Outliers

- Definition: data values that are much larger or smaller than the rest of the values in the dataset.

- We will look at the standardized residuals, $$ e_{i_{\text{standardized}}} = \frac{e_i}{\sqrt{\text{MSE}(1-h_i)}}, $$ where

    - $e_i = y_i - \hat{y}_i$ is the residual of the $i$^th^ observation
    - $h_i$ is the leverage of the $i$^th^ observation
    
- If $|e_{i_{\text{standardized}}}| > 2.5 \ \to \ $ outlier.

- If $|e_{i_{\text{standardized}}}| > 3 \ \to \ $ extreme outlier.

## Outliers: R Syntax

- We will use the `rstandard()` function to find the residuals.

- For ease of examining in large datasets, we will use it to create a "flag."

```{r, echo = TRUE, eval = FALSE}
[dataset] <- [dataset] %>%
  mutate(outlier = abs(rstandard([m]))>2.5)
```

- We can count the number of outliers,

```{r, echo = TRUE, eval = FALSE}
[dataset] %>% count(outlier)
```

- We can just look at outliers from the dataset,

```{r, echo = TRUE, eval = FALSE}
[new dataset] <- [dataset] %>% 
  filter(outlier == TRUE)
```

- We can also exclude outliers from the dataset,

```{r, echo = TRUE, eval = FALSE}
[new dataset] <- [dataset] %>% 
  filter(outlier == FALSE)
```

## Example

- Recall the penguin data and the model,

```{r, echo = TRUE}
data <- palmerpenguins::penguins
m1 <- lm(body_mass_g ~ bill_length_mm + flipper_length_mm, data = data)
summary(m1)
```

$$ \hat{y} = -5536.90 + 6.05 \text{ bill} + 48.15 \text{ flipper} $$

## Outliers: Example

- Let's check for outlier penguins,

```{r, echo = TRUE, error = TRUE}
library(tidyverse)
data <- data %>% mutate(outlier = abs(rstandard(m1))>2.5)
```

- Uh oh! This doesn't work!

- R is telling us that we are trying to mutate a dataset with 344 observations using a model that was constructed on 342 observations.

    - Two penguins were excluded from the model due to missing data.

## Outliers: Example

- Let's try this again,

::: {.panel-tabset}

## Code
```{r, echo = TRUE, eval = FALSE}
data2 <- palmerpenguins::penguins %>% 
  select(body_mass_g, bill_length_mm, flipper_length_mm) %>%
  na.omit() %>% 
  mutate(obs = row_number())

m1 <- lm(body_mass_g ~ bill_length_mm + flipper_length_mm, data = data2)
summary(m1)
```

## Output
```{r, echo = FALSE, eval = TRUE}
data2 <- palmerpenguins::penguins %>% 
  select(body_mass_g, bill_length_mm, flipper_length_mm) %>%
  na.omit() %>% 
  mutate(obs = row_number())

m1 <- lm(body_mass_g ~ bill_length_mm + flipper_length_mm, data = data2)
summary(m1)
```

:::

$$ \hat{y} = -5536.90 + 6.05 \text{ bill} + 48.15 \text{ flipper} $$

## Outliers: Example

- Let's again check for outlier penguins,

```{r, echo = TRUE}
data2 <- data2 %>% mutate(outlier = abs(rstandard(m1))>2.5)
head(data2)
```

## Outliers: Example

- How many penguins are outliers?

```{r, echo = TRUE}
data2 %>% count(outlier)
```

## Outliers: Example

- Let's look at the outliers,

```{r, echo = TRUE}
data3 <- data2 %>% filter(outlier == TRUE)
head(data3)
```

## Influence and Leverage

- Influence: A measure of how much the slope is affected by the observation.

- Leverage: A measure of the "extremeness" of an observation with respect to the independent variables.

- Cook's Distance ($d_i$): Measures the extent to which the estimates of the regression coefficients change when an observation is deleted from the analysis.

$$ d_i = \left( \frac{1}{k+1} \right) \left( \frac{h}{1-h_i} \right) e_{i_{\text{standardized}}} $$

- We will use the graphical assessment and look for "spikes."

## Influence and Leverage: R Syntax

- We will use the [`gg_cooksd()`](https://www.rdocumentation.org/packages/lindia/versions/0.9/topics/gg_cooksd) function from the [`lindia`](https://www.rdocumentation.org/packages/lindia/versions/0.9) package.

```{r, echo = TRUE, eval = FALSE}
gg_cooksd([m]) 
```

- Note that this is built in `ggplot()` -- you can add on as you wish

## Influence and Leverage: Example

- Let's find the Cook's distance graph for the penguin model.

```{r, echo = TRUE}
library(lindia)
gg_cooksd(m1) + theme_bw()
```

- I would investigate points 39, 169, 292, and 313.

## Multicollinearity

- Collinearity/multicollinearity: a correlation between two or more predictor variables affects the estimation procedure.

- We will use the variance inflation factor (VIF) to check for multicollinearity.

$$ \text{VIF}_j = \frac{1}{1-R^2_j}, $$

- where

    - $j$ = the predictor of interest and $j \in \{1, 2, ..., k \}$,
    - $R^2_j$ results from regressing $X_j$ on the remaining $(k-1)$ predictors.
    
- We say that multicollinearity is present if VIF > 10.

## Multicollinearity

- How do we deal with multicollinearity?

    - Easy answer: remove at least one predictor from the collinear set, then reassess VIF.
    
    - More complicated: how do we know which predictor should be the one removed?
    
        - (We will likely need to consult with the research team.)
        
- Notes for more advanced models:

    - If categorical predictors are present in the model, multicollinearity may be difficult to assess.
    
    - We <u>must not</u> include interaction terms when assessing VIF.
    
## Multicollinearity: R Syntax

- We will use the [`vif()`](https://www.rdocumentation.org/packages/car/versions/3.1-0/topics/vif) function from the `car` package.

```{r, echo = TRUE, eval = FALSE}
vif([m])
```

- There will be a value for each predictor in the model.

- Warning! If outside of linear regression using the normal distribution, the `vif()` function in R is not reliable. (Story time!)
    
## Multicollinearity: Example

- Let's check the multicollinearity for our penguin model,

```{r, echo = TRUE, warning = FALSE}
library(car)
vif(m1)
```

- No multicollinearity is present.

## Multicollinearity: Example

```{r, echo = TRUE}
m2 <- lm(body_mass_g ~ bill_length_mm + flipper_length_mm + bill_length_mm:flipper_length_mm, data = data)
vif(m2)
```

## Sensitivity Analysis

- We can remove problem datapoints and reanalyze the data to see "how different" the results are. 

- Let's remove the problem datapoints from the penguin dataset and reanalyze.

```{r, echo = TRUE}
data4 <- data2 %>%
  filter(outlier == FALSE & !obs %in% c(39, 169, 292, 313)) 
m3 <- lm(body_mass_g ~ bill_length_mm + flipper_length_mm, data = data4)
summary(m3)
```

$$ \hat{y} = -5752.56 + 5.48 \text{ bill} + 48.29 \text{ flipper} $$

## Sensitivity Analysis

$$ \hat{y} = -5536.90 + 6.05 \text{ bill} + 48.15 \text{ flipper} $$

- Bill length is not significant (*p* = 0.244).
- Flipper length is significant (*p* < 0.001).


$$ \hat{y} = -5752.56 + 5.48 \text{ bill} + 48.29 \text{ flipper} $$

- Bill length is not significant (*p* = 0.281).
- Flipper length is significant (*p* < 0.001).






## Wrap Up

- Today we have covered the basics of linear regression.

- When you move to other courses, you will learn more about linear regression - we have barely scratched the surface!

    - Categorical predictors.
    - Interaction terms.
    - Non-normal outcomes (categorical, count, skewed continuous).
    
- Remember that ANOVA = regression and regression = ANOVA!

    - We are just changing how we display the data/results.
    
    - Because of this, we already know how to assess the assumptions and was not discussed .

- Next week we will discuss categorical analysis.




