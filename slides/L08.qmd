---
title: "ANOVA Assumptions and the Kruskal-Wallis"
subtitle: "STA5176: Statistical Modeling"
author: "Dr. Seals"
format: 
  revealjs:
    theme: dark2
    self-contained: false
    slide-number: false
    footer: "[STA5176 - Statistical Modeling](https://samanthaseals.github.io/STA5176)"
    width: 1600
    height: 900
    chalkboard: true
    df-print: paged
    html-math-method: katex
editor: source
---

## Introduction

- In the last lecture, we were introduced to one-way ANOVA, an extension of the two-sample *t*-test.

- Today, we will talk about the assumptions on ANOVA, and what to do when the assumptions are broken.

## ANOVA Assumptions: Definition

- We can represent ANOVA with the following model:
$$ y_{ij} = \mu + \tau_i + \varepsilon_{ij} $$

- where:

    - $y_{ij}$ is the $j^{\text{th}}$ observation in the $i^{\text{th}}$ group,
    - $\mu$ is the overall (grand) mean,
    - $\tau_i$ is the treatment effect for group $i$, and
    - $\varepsilon_{ij}$ is the error term for the $j^{\text{th}}$ observation in the $i^{\text{th}}$ group.
    
## ANOVA Assumptions: Definition

- We assume that the error term follows a normal distribution with mean 0 and a constant variance, $\sigma^2$. i.e.,
$$\varepsilon_{ij} \overset{\text{iid}}{\sim} N(0, \sigma^2)$$

- Very important note: **the assumption is on the error term** and NOT on the outcome!

- We will use the residual (the difference between the observed value and the predicted value) to assess assumptions:
$$ e_{ij} = y_{ij} - \hat{y}_{ij} $$

## ANOVA Assumptions: Normality
    
- The histogram of residuals:

    - Should be mound-shaped and symmetric
    - Should be centered at $\mu=0$
    - Note: it may look "wonky" when sample sizes are small
    
- The quantile-quantile plot:

    - Should have points close to the 45$^\circ$ line
    - We will focus on the "center" portion of the plot

- I always base my final decision on the NPP

## ANOVA Assumptions: Constant Variance

- The scatterplot of residual ($e_{ij}$) vs. predicted ($\bar{y}_i$):

    - We assess the length of the line -- we want them to be approximately equal
    
- Brown-Forsyth-Levine (BFL) test for homogeneity of variances:

    - From Chapter 7, recall that we construct ANOVA for the transformed data,
$$z_{ij} = | y_{ij} - \tilde{y}_i|$$

## ANOVA Assumptions: R Syntax

- We will use plots from base R.

```{r, echo = TRUE, eval = FALSE}
# scatterplot for assessing variance
plot(fitted([m]), resid([m]))

# histogram for assessing normality
hist(resid([m]))

# q-q plot for assessing normality
qqnorm(resid([m])) # plot itself
qqline(resid([m])) # add the 45 degree line
```

## ANOVA Assumptions: Penguins

- Recall the penguin example from our last lecture -- we determined that bill length varied by species (*p* < 0.001).

```{r, echo = TRUE}
penguins <- palmerpenguins::penguins
m1 <- lm(bill_length_mm ~ species, data = penguins)
anova(m1)
```

## ANOVA Assumptions: Penguins

- Scatterplot of residuals (*y*-axis) against predicted (*x*-axis)
```{r, echo = TRUE}
plot(fitted(m1), resid(m1))
```

## ANOVA Assumptions: Penguins

- Histogram of residuals
```{r, echo = TRUE}
hist(resid(m1))
```

## ANOVA Assumptions: Penguins

- Quantile-quantile plot
```{r, echo = TRUE}
qqnorm(resid(m1))
qqline(resid(m1)) 
```

## ANOVA Assumptions: Insulation Shields

- A small corporation makes insulation shields for electrical wires using three different types of machines. A quality engineer at the corporation randomly selects shields produced by each of the machines and records the inside diameter of each shield (in millimeters). Using ANOVA to test at $\alpha=0.05$, she determines that there is not a difference (*p* = 0.094).

```{r, echo = TRUE}
library(gsheet)
library(tidyverse)
data <- as_tibble(gsheet2tbl("https://docs.google.com/spreadsheets/d/1Bqv7WaV7AUoMs7HWhSHvJR0rZLFGnDDOjwuIHKfrAqY/edit#gid=0"))
long <- data %>%
  pivot_longer(cols = c("Machine A", "Machine B", "Machine C"),
               names_to = "Machine",
               values_to = "Diameter")
m2 <- lm(Diameter ~ Machine, data = long)
anova(m2)
```

## ANOVA Assumptions: Insulation Shields

- Scatterplot of residuals (*y*-axis) against predicted (*x*-axis)
```{r, echo = TRUE}
plot(fitted(m2), resid(m2))
```

## ANOVA Assumptions: Insulation Shields

```{r, echo = TRUE}
long %>%group_by(Machine) %>% summarize(sd(Diameter, na.rm = TRUE)) %>% ungroup()
```

```{r, echo = TRUE}
library(car)
leveneTest(Diameter ~ Machine, data = long, center = median)
```

## ANOVA Assumptions: Insulation Shields

- Histogram of residuals
```{r, echo = TRUE}
hist(resid(m2))
```

## ANOVA Assumptions: Insulation Shields

- Quantile-quantile plot
```{r, echo = TRUE}
qqnorm(resid(m2))
qqline(resid(m2)) 
```

## Kruskal-Wallis: Definition

- The Kruskal-Wallis is the nonparametric alternative to one-way ANOVA

- The data is ranked without regard to grouping (similar to the Wilcoxon rank sum).

    - If ties are present, we will assign the average rank.
    
- In computation:

    - We are interested in $R_i$, the sum of ranks for group $i$
    
    - We also need the number of tied groups ($g$), indexed by $j$, and the number of observations in each tied group, $t_j$.

## Kruskal-Wallis: No Ties

**Hypotheses**

  - $H_0: \ M_1 =  ... = M_k$
  - $H_1:$ at least one $M_i$ is different
  
**Test Statistic**

  - $H^* = \frac{12}{n(n+1)} \sum_{i=1}^k \frac{R_i^2}{n_i} - 3(n+1) \sim \chi^2_{k-1}$ 

**Rejection Region**

  - Reject $H_0$ if $p < \alpha$
  
## Kruskal-Wallis: Ties

**Hypotheses**

  - $H_0: \ M_1 =  ... = M_k$
  - $H_1:$ at least one $M_i$ is different
  
**Test Statistic**

  - $H = \frac{H^*}{1- \frac{\sum_{j=1}^g (t_j^3 - t_j)}{n^3-n}} \sim \chi^2_{k-1}$ 

**Rejection Region**

  - Reject $H_0$ if $p < \alpha$

## Kruskal-Wallis: R Syntax

- We will use the [`kruskal.test()`](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/kruskal.test) function to perform the Kruskal-Wallis test.

```{r, echo = TRUE, eval = FALSE}
kruskal.test([outcome variable] ~ [grouping variable], 
             data = [dataset])
```

## Kruskal-Wallis: Insulation Shields

- Let us now apply the Kruskal-Wallis test to the insulation shield data from earlier,

```{r, echo = TRUE}
kruskal.test(Diameter ~ Machine, data = long)
```

## Kruskal-Wallis: Insulation Shields {.smaller}

**Hypotheses**

-   $H_0: \ M_{\text{A}} = M_{\text{B}} = M_{\text{C}}$
-   $H_1:$ at least one is different

**Test Statistic**

-   $H = 9.891$

***p*****-Value**

-   $p = 0.007$

**Conclusion/Interpretation**

-   Reject $H_0$ at the $\alpha=0.05$ level. There is sufficient evidence to suggest that the average diameter is different between the three machines.

## Kruskal-Wallis Post-hoc: Definition

- Like when using ANOVA, we can apply post-hoc testing procedures to the Kruskal-Wallis.

- Instead of using $|\bar{y}_i - \bar{y}_j|$, we will use $|\bar{R}_i - \bar{R}_j|$, where $\bar{R}_i$ is the average rank of group $i$.

- The comparison as defined in the textbook:

    - We declare $M_i \ne M_j$ if $|\bar{R}_i - \bar{R}_j| \ge KW$, where
    
$$ KW = \frac{q_{\alpha}(k, \infty)}{\sqrt{2}} \sqrt{\frac{n(n+1)}{12} \left( \frac{1}{n_i} + \frac{1}{n_j} \right)} $$
- $q_{\alpha}(k, \infty)$ is the critical value from the Studentized range distribution.

## Kruskal-Wallis Post-hoc: R Syntax

- We will use the [`kruskalmc()`](https://www.rdocumentation.org/packages/pgirmess/versions/2.0.0/topics/kruskalmc) function from the [`pgirmess` package](https://www.rdocumentation.org/packages/pgirmess/versions/2.0.0) to perform the Kruskal-Wallis post-hoc test.

```{r, echo = TRUE, eval = FALSE}
kruskalmc([outcome variable] ~ [grouping variable], 
          data = [dataset])
```

## Kruskal-Wallis Post-hoc: Insulation Shields

- Let us now apply the Kruskal-Wallis post-hoc test to the insulation shield data from earlier,

```{r, echo = TRUE}
library(pgirmess)
kruskalmc(Diameter ~ Machine, data = long)
```

- Some summary statistics to help with interpretations/understanding:

    - Machine A: M<sub>A</sub> = `r median(data$"Machine A", na.rm = TRUE)`, IQR<sub>A</sub> = `r IQR(data$"Machine A", na.rm = TRUE)`
    - Machine B: M<sub>B</sub> = `r median(data$"Machine B", na.rm = TRUE)`, IQR<sub>B</sub> = `r IQR(data$"Machine B", na.rm = TRUE)`
    - Machine C: M<sub>C</sub> = `r median(data$"Machine C", na.rm = TRUE)`, IQR<sub>C</sub> = `r IQR(data$"Machine C", na.rm = TRUE)`


- Only Machines A and C are significantly different.

- Machine B is not significantly different from either Machines A or C.



