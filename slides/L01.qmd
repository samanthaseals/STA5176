---
title: "Review of Estimation and Inference"
subtitle: "STA5176: Statistical Modeling"
author: "Dr. Seals"
format: 
  revealjs:
    theme: dark2
    self-contained: false
    slide-number: false
    footer: "[STA5176 - Statistical Modeling](https://samanthaseals.github.io/STA5176)"
    width: 1600
    height: 900
    chalkboard: true
    df-print: paged
    html-math-method: katex
editor: source
---

```{r setup_environment, include = FALSE}
library("tidyverse") 
```


## Describing Data

-   **parameter**: descriptive measure for a population

    -   e.g., $\mu$, $\sigma^2$

-   **statistic**: descriptive measure for a sample

    -   e.g., $\bar{x}$, $s^2$

## `summarize()` in `tidyverse`

-   To find summary statistics, we will use the [`summarize()`](https://dplyr.tidyverse.org/reference/summarise.html) function from the [`tidyverse`](https://www.tidyverse.org/) package.
    -   Technically, we are using the [`dplyr`](https://dplyr.tidyverse.org/) package, but it is included in the [`tidyverse`](https://www.tidyverse.org/) package.
-   In particular, we are interested in the mean, median, variance, standard deviation, and interquartile range.
-   You can see more information on the official [`summarize()`](https://dplyr.tidyverse.org/reference/summarise.html) website.

## Palmer Penguins

<div style = "font-size: .75em; margin-top: 2em">

```{r}
penguins <- palmerpenguins::penguins
set.seed(1)
penguins |>
 slice_sample(prop = 1)
```

## Central Tendency - Mean

-   sample mean,
$$\bar{x} = \frac{\sum_{i=1}^n x_i}{n}$$

```{r, echo = TRUE}
penguins %>% 
  summarize(mean(body_mass_g, na.rm = TRUE))
```


## Central Tendency - Median

-   sample median, \begin{align*}
    \text{median} &= 50^{\text{th}} \text{ percentile} \\ &= \text{value that divides the dataset in half}
    \end{align*}


```{r, echo = TRUE}
penguins %>% 
  summarize(median(body_mass_g, na.rm = TRUE))
```


## Variability - Variance

-   sample variance,
$$ s^2 = \frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n-1} $$

```{r, echo = TRUE}
penguins %>% 
  summarize(var(body_mass_g, na.rm = TRUE))
```


## Variability - Standard Deviation

-   sample standard deviation,
$$s = \sqrt{\frac{\sum_{i=1}^n (x_i - \bar{x})^2}{n-1}}$$

```{r, echo = TRUE}
penguins %>% 
  summarize(sd(body_mass_g, na.rm = TRUE))
```


## Variability - Interquartile Range

-   interquartile range (IQR),
$$ \text{IQR} = 75^{\text{th}} \text{ percentile} - 25^{\text{th}}\text{ percentile} $$

```{r, echo = TRUE}
penguins %>% 
  summarize(IQR(body_mass_g, na.rm = TRUE))
```


## Summaries with `group_by()`

-   We can use the [`group_by()`](https://dplyr.tidyverse.org/reference/group_by.html) function to request summaries by groups,


```{r, echo = TRUE}
penguins %>% 
  group_by(species) %>%
  summarize(mean = mean(body_mass_g, na.rm = TRUE),
            sd = sd(body_mass_g, na.rm = TRUE),
            median = median(body_mass_g, na.rm = TRUE),
            IQR = IQR(body_mass_g, na.rm = TRUE))
```


## Confidence Intervals -- Conceptually

-   In addition to the actual estimator, we are interested in how "good" our estimation (e.g., $\bar{x}$ and $s$) is.

-   **Confidence intervals** (CIs) allow us to determine this.

    -   CIs are interval estimators that describe the uncertainty about the relation between the sample statistic and the population parameter.

-   Suppose we have the following estimation:

    -   $\bar{x} =$ 5; 95% CI for $\mu$: (--4, 14)
    -   $\bar{y} =$ 2; 95% CI for $\mu$: (0.5, 3.5)

## Confidence Intervals -- Application

-   We can use CIs to:

    -   Determine if a specific value (e.g., 0) is plausible.
    -   Determine if a difference exists between two groups.

-   Suppose we have the following estimation:

    -   $\bar{x} =$ 5; 95% CI for $\mu$: (--4, 14)
    -   $\bar{y} =$ 2; 95% CI for $\mu$: (0.5, 3.5)
    -   $\bar{x_1} - \bar{x}_2 =$ 3; 95% CI for $\mu_1 - \mu_2$: (1, 5)
    -   $\frac{s_1}{s_2} =$ 1.5; 95% CI for $\frac{\sigma_1}{\sigma_2}$: (0.5, 2.5)

## Confidence Intervals -- Widths

-   Two things will affect the width of the CI:

    -   confidence level
    -   sample size

-   Increasing the confidence level makes the interval wider.

-   Increasing the sample size makes the interval more narrow.

## Hypothesis Testing -- Introduction

-   Hypothesis testing has several components:

    -   Hypotheses
    -   Test statistic
    -   $p$-value
    -   Rejection region
    -   Conclusion
    -   Interpretation

## Hypothesis Testing -- Hypotheses

-   There are two hypotheses:

    -   The null, $H_0$.
    -   The alternative, $H_1$.

-   The null hypothesis is the "status quo" and the "opposite" of the alternative.

-   The alternative hypothesis is the research question at hand.

-   Together, $H_0$ and $H_1$ cover $\mathbb{R}$.

## Hypothesis Testing -- Hypotheses

-   Hypothesis tests are categorized into direction of "tail" for the alternative ($H_1$).

    -   Left-tailed test: $<$
    -   Right-tailed test: $>$
    -   Two-tailed test: $\ne$

-   Hypothesis tests are also categorized into how many "tails" the alternative ($H_1$) falls into.

    -   One-tailed test: $<$ or $>$
    -   Two-tailed test: $\ne$

## Hypothesis Testing -- Hypotheses

-   Example:

    -   Is the penguin colony laying more than 100 eggs per year?

        -   $H_0$: $\mu \le 100$
        -   $H_1$: $\mu > 100$

-   Example:

    -   Are the orange trees producing more fruit than the apple trees?

        -   $H_0$: $\mu_{\text{orange}} = \mu_{\text{apple}}$
        -   $H_1$: $\mu_{\text{orange}} \ne \mu_{\text{apple}}$

## Hypothesis Testing -- Test Statistic

-   The test used depends on the research question.

    -   Testing one or two means? $t$-test
    -   Testing two categorical variables for independence? $\chi^2$

-   The test statistic will be defined by the specific test needed.

-   The value of the test statistic depends on the observed data.

    -   If we perform the experiment a second time, we would likely observe a different test statistic.

## Hypothesis Testing -- ***p***-Value

-   ***p*****-value**: the probability of observing what we've observed or something more extreme *assuming that the null hypothesis is true*.

-   This allows us to quantify the evidence supporting the alternative hypothesis.

    -   Remember that the null hypothesis is the "status quo" -- it will be believed unless there is enough evidence to support the alternative.

-   The *p*-value depends on (1) the distribution of the test statistic, (2) the value of the test statistic, (3) the direction/tails of the test.

## Hypothesis Testing -- Rejection Region

-   The rejection region is the criteria for which we believe either the null or the alternative hypothesis.

-   When using $p$-values, the rejection region is always the same.

    -   Reject $H_0$ if $p < \alpha$, where $\alpha$ is the probability of incorrectly rejecting $H_0$.

        -   Typical $\alpha$ in Happy Textbook World: 0.01, 0.05, and 0.10.
        -   Usual $\alpha$ in real life: 0.05.

-   This class will **only** use the $p$-value approach. We will not be using critical values or statistical tables.

## Hypothesis Testing -- Conclusions

-   If $p < \alpha$, we **reject** $H_0$.

-   If $p \ge \alpha$, we **fail to reject** (FTR) $H_0$.

-   [We never (ever, *ever*, ***ever***) accept the null hypothesis.]{style="color: red;"}

## Hypothesis Testing -- Interpretations

-   Because we are often working (collaborating) with non-quantitative people, we should provide a summary statement that is easily understandable, in terms of the original research question.

-   There \[is / is not\] sufficient evidence to suggest that \[$H_1$ in words\].

    -   "is" if we reject
    -   "is not" if we FTR
    -   do not use formal math notation


