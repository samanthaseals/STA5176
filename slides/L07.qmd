---
title: "Linear Regression and Correlation"
subtitle: "STA5176 Lecture 7, Summer 2023"
execute:
  echo: true
  warning: false
  message: false
format: 
  revealjs:
    theme: uwf
    self-contained: true
    slide-number: false
    footer: "[STA5176 - Statistical Modeling](https://samanthaseals.github.io/STA5176)"
    width: 1600
    height: 900
    df-print: paged
    html-math-method: katex
editor: source
---

## Introduction

- Previously discussed analyzing continuous data

    - Two-sample *t*-test
    - One-way ANOVA
    - Two-way ANOVA
    
- Commonality: comparing groups

- New: thinking about this in model form; continuous predictor

## Introduction

- **Response variable (outcome)**: the dependent variable; what we are trying to model the outcome of; *y*

- **Explanatory variable (predictor)**: the independent variable; what we are using to model the outcome; *x*

- Nomenclature: 

    - -variate: describes number of outcomes
    - -variable: describes number of predictors

- Special case of regression: simple linear regression

    - One continuous outcome
    - One continuous predictor
    
## Prediction Equation: Population

- The prediction equation for the **population**: $$ y = \beta_0 + \beta_1 x $$

- $\beta_0$ and $\beta_1$ are **parameters**

    - $\beta_0$ is the $y$-intercept
    - $\beta_1$ is the slope

- $y$ is the observed outcome
    
## Prediction Equation: Sample

- The prediction equation for the **sample**: $$ \hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x $$

- $\hat{\beta}_0$ and $\hat{\beta}_1$ are **statistics**

    - $\hat{\beta}_0$ is the estimate of the $y$-intercept
    - $\hat{\beta}_1$ is the estimate of the slope

- $y$ is the predicted outcome

## Prediction Equation: Graphically

<center><img src="images/L11a.png" width = 1200></center><br>

- "line of best fit"

## Linear Regression: Introduction

- We are using data to quantitatively explain the relationship between $x$ and $y$.

    - As $x$ increases, what happens to $y$?
    
- Linear regression assumes linearity. 

    - $\beta_1$ is constaint: as $x$ increases, the slope does not change.
    
## Linear Regression: Introduction

- Note that $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$ is an exact function.

    - There is no room for error!

<center><img src="images/L11b.png" width = 1000></center>

## Linear Regression: Introduction

- Thus, we will add an error term:

\begin{align*}
  \text{Population: } & y = \beta_0 + \beta_1 x + \varepsilon \\
  \text{Sample: } & y = \hat{\beta}_0 + \hat{\beta}_1 x + e
\end{align*}

- where $\varepsilon$ and $e$ are the error terms that measure the difference between the expected/predicted value and the observed value.

- Note that:

    - ($\beta_0 + \beta_1 x$) and ($\hat{\beta}_0 + \hat{\beta}_1 x$) are the predictable parts of the model,
    - $\varepsilon$ and $e$ are the unpredictable parts of the models,
    - and $\varepsilon \overset{\text{iid}}{\sim} N(0, \sigma^2)$

## Linear Regression: Assumptions

- Remember the ANOVA assumptions? (ANOVA = regression!)

$$\varepsilon \overset{\text{iid}}{\sim} N(0, \sigma^2)$$

- The <u>**residuals**</u> follow a normal distribution

    - Mean, $\mu=0$
    - Common variance, $\sigma^2$

- In R, we assess the assumptions as we did in ANOVA.

## Linear Regression: Example

- Consider a basic example: the cost of road resurfacing. 

    - *y* is the cost in thousands of dollars,
    - *x* is the mileage
    
<center><img src="images/L11c.png" width = 800></center>

## Linear Regression: Computation

- We will estimate the population regression line using the sample regression line,

$$\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x$$

- We will use ordinary least squares to estimate regression coefficients.

$$ \hat{\beta}_1 = \frac{S_{xy}}{S_{xx}} \ \ \ \text{ and} \ \ \ \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x} $$

- where

$$ S_{xx} = \sum_{i=1}^n x_i^2 - \frac{(\sum_{i=1}^n x_i)^2}{n} \ \ \ \text{ and} \ \ \ S_{xy} = \sum_{i=1}^n x_i y_i - \frac{\sum_{i=1}^n x_i \sum_{i=1}^n y_i}{n} $$

## Linear Regression: R Syntax

- We used the `lm()` function in ANOVA -- we will use it again for linear regression (ANOVA = regression!)

```{r, echo = TRUE, eval = FALSE}
m <- lm([outcome] ~ [predictor],
        data = [dataset])
```

- To see the regression information, we will run the result through the `summary()` function.

```{r, echo = TRUE, eval = FALSE}
summary(m)
```

## Linear Regression: Example

- Let's find the regression line for the road resurfacing data. 

```{r}
library(tidyverse)
x <- c(1, 3, 4, 5, 7)
y <- c(6, 14, 10, 14, 26)
data <- tibble(x, y)
```

```{r, echo = TRUE}
m1 <- lm(y ~ x, data = data)
summary(m1)
```

$$ \hat{y} = 2 + 3x $$

## Linear Regression: Example

<center><img src="images/L11d.png" width = 1200></center>

## Linear Regression: Interpretations

- We must interpret the regression coefficients. 

    - Intercept: when [predictor = 0], the average [outcome] is [$\hat{\beta}_0$].

    - Slope: For a 1 [units of predictor] increase in [predictor], we expect [outcome] to [increase or decrease] by [$|\hat{\beta}_1|$] [units of outcome].
    
        - If $\hat{\beta}_1 > 0$, there is an increase.
        - If $\hat{\beta}_1 < 0$, there is a decrease.
        
## Linear Regression: Example

- From the road surfacing example,

$$ \hat{y} = 2 + 3x $$

- Intercept: before considering mileage, we expect road resurfacing will cost $2,000.

- Slope: for every mile resurfaced, we expect the cost to increase by $3,000.

- Slope: for every 10 miles resurfaced, we expect the cost to increase by $30,000.

## Linear Regression: Testing Slope

- **Hypotheses**

  - $H_0: \beta_1 = \beta_1^{(0)}$
  - $H_1: \beta_1 \ne \beta_1^{(0)}$


- **Test Statistic**

    - $$ t_0 = \frac{\hat{\beta}_1 - \beta_1^{(0)}}{\text{SE}_{\hat{\beta}_1}} $$ where

        -   $\beta_1^{(0)}$ is the hypothesized value (often 0)
        -   $\text{SE}_{\hat{\beta}_1} = \sqrt{\text{MSE}/S_{xx}}$
    
- ***p*-Value**

    - $p = 2 \times P[t_{1, 1 - \alpha/2} \ge t_0]$
    
## Linear Regression: Example

- Recall the road resurfacing example.

```{r, echo = TRUE}
summary(m1)
```

## Linear Regression: Example

- **Hypotheses**

    - $H_0: \beta_1 = 0$
    - $H_1: \beta_1 \ne 0$

- **Test Statistic and *p*-value**

    - $t_0 = 3.503$

    - $p = 0.0394$
    
- **Rejection Region**

    - Reject $H_0$ if $p < \alpha$; $\alpha = 0.05$

- **Conclusion/Interpretation**

    - Reject $H_0$. There is sufficient evidence to suggest that mileage is a significant predictor of cost.

## Linear Regression: Confidence Interval for Slope

**Confidence Interval for $\beta_1$:**

$$\hat{\beta}_1 \pm t_{\alpha/2, n-2} \text{SE}_{\hat{\beta}_1}$$

- We will use the `confint()` function to find the confidence interval.

```{r, echo = TRUE}
confint(m1)
```

- The 95% CI for $\beta_1$ is (0.27, 5.73)

```{r, echo = TRUE}
confint(m1, level = 0.99)
```

- The 99% CI for $\beta_1$ is (-2.00, 8.00)

## Correlation: Pearson's Introduction

- Unfortunately, $\beta_1$ does not tell us the *strength* of the relationship between *x* and *y*.

    - This is because of the units involved both on *x* and *y*.

- We will now look at a unitless measurement, called correlation.

    - Correlation: a unitless measurement of the strength of the **linear** relationship between two variables.
    
- The stronger the relationship, the better *x* predicts *y* in $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1x$.

## Correlation: Pearson's Introduction

- The population correlation is $\rho$ ("rho").

- The sample correlation is $r$.

- Correlation $\in$ [-1, 1]

    - The closer to +/- 1, the stronger the relationship.
    - $\rho>0$: positive correlation; as *x* increases, *y* increases
    - $\rho<0$: negative correlation; as *x* increases, *y* decreases
    - $\rho=0$: no correlation; as *x* increases, *y* is constant

## Correlation: Pearson's Computation

- Given $n$ pairs of observations $(x_i, y_i)$, Pearson's correlation is as follows:

$$ r = \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}, $$

- where:

$$ S_{xx} = \sum_{i=1}^n x_i^2 - \frac{(\sum_{i=1}^n x_i)^2}{n} \ \ \ \text{ and} \ \ \ S_{yy} = \sum_{i=1}^n y_i^2 - \frac{(\sum_{i=1}^n y_i)^2}{n}  $$

$$S_{xy} = \sum_{i=1}^n x_i y_i - \frac{\sum_{i=1}^n x_i \sum_{i=1}^n y_i}{n} $$

## Correlation: Pearson's R Syntax

- We will use the `cor()` function to find the correlation.

```{r, echo = TRUE, eval = FALSE}
cor([dataset]$[var1], [dataset]$[var2], 
    use = "complete.obs") # single correlation

cor(dataset, 
    use = "complete.obs") # correlation matrix
```

- While we can find the correlation matrix for 

- Note that there are several other packages/functions out there to help with visualization of correlation!

## Correlation: Example

- Let's revisit the penguins dataset and find the correlation matrix.

```{r, echo = TRUE}
data <- palmerpenguins::penguins %>%
  select(bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g)
cor(data, use = "complete.obs") 
```

- Let's look at just the correlation between bill length and flipper length.

```{r, echo = TRUE}
cor(data$bill_length_mm, data$flipper_length_mm, use = "complete.obs")
```


## Correlation: Confidence Interval

**Confidence Interval for $\rho$**

$$ \left(\frac{e^{2z_1}-1}{e^{2z_1}+1}, \frac{e^{2z_2}-1}{e^{2z_2}+1} \right), $$

- where:

$$ z_1 = z- \frac{z_{\alpha/2}}{\sqrt{n-3}} \ \ \ \text{ and} \ \ \ z_2 = z+ \frac{z_{\alpha/2}}{\sqrt{n-3}} $$

- and: 

$$ z = \frac{1}{2} \ln\left(\frac{1+r}{1-r} \right)$$

## Correlation: R Syntax

- We will now use the `cor.test()` function.

```{r, echo = TRUE, eval = FALSE}
cor.test([dataset]$[var1], [dataset]$[var2], 
         conf.level = [level],
         use = "complete.obs")
```

## Correlation: Example

```{r, echo = TRUE}
cor.test(data$bill_length_mm, data$flipper_length_mm, use = "complete.obs")
```

- The 95% CI for $\rho$ is (0.59, 0.71).

```{r, echo = TRUE}
cor.test(data$bill_length_mm, data$flipper_length_mm, conf.level = 0.99, use = "complete.obs")
```

- The 99% CI for $\rho$ is (0.57, 0.73).

## Correlation: Assumptions

- Note that the assumption on Pearson's correlation is that **both** variables are normally distributed.

    - But... in regression, I don't "care" about the distribution of *x*!
    
    - And... we also don't "care" about the distribution of *y*.
    
    - (We only care about the distribution of the residuals.)
    
- Thus, need to check assumption of normality.

    - I use histograms for quick and dirty check. 
    
    - We could use q-q plots...
    
## Correlation: Example

```{r, echo = FALSE}
library(ggpubr)
p1 <- data %>% ggplot(aes(sample = bill_length_mm)) +
  stat_qq(size=3) +
  stat_qq_line() +
  theme_minimal() +
  xlab("Theoretical") +
  ylab("Sample") +
  ggtitle("Bill Length") +
  theme(text = element_text(size=20))
p2 <- data %>% ggplot(aes(sample = flipper_length_mm)) +
  stat_qq(size=3) +
  stat_qq_line() +
  theme_minimal() +
  xlab("Theoretical") +
  ylab("Sample") +
  ggtitle("Flipper Length") +
  theme(text = element_text(size=20))
ggarrange(p1, p2, nrow = 1)
```


## Correlation: Spearman's Introduction

- If we do not meet the assumption for Pearson's correlation, we can use Spearman's correlation.

    - This is a nonparametric method to finding the corerlation coefficient.
    
- Algorithm:

    - 1. Sort by *x*, assign ranks (*R<sub>x</sub>*)
    - 2. Sort by *y*, assign ranks (*R<sub>y</sub>*)
    - 3. Compute Pearson's on (*R<sub>x</sub>*, *R<sub>y</sub>*)
    
- Note that we are transforming (*x*, *y*) $\to$ (*R<sub>x</sub>*, *R<sub>y</sub>*)

## Correlation: Spearman's R Syntax

- We again use the `cor.test()` function, but specify Spearman's.

```{r, echo = TRUE, eval = FALSE}
cor([dataset]$[var1], [dataset]$[var2], 
    use = "complete.obs", 
    method = "spearman") # single correlation

cor(dataset, 
    use = "complete.obs", 
    method = "spearman") # correlation matrix
```


## Correlation: Example

- Let's revisit the penguins dataset and find the correlation matrix.

```{r, echo = TRUE}
cor(data, use = "complete.obs", method = "spearman") 
```

- Let's look at just the correlation between bill length and flipper length.

```{r, echo = TRUE}
cor(data$bill_length_mm, data$flipper_length_mm, use = "complete.obs", method = "spearman")
```

## Model Visualization

- While we are in simple linear regression, visualization is easy. 

- We will first plot a scatterplot, then overlay the regression line.

    - The predictor will be on the *x*-axis.
    - The outcome will be on the *y* axis.
    - We will create predicted values to construct the line.
    
## Model Visualization: Example

- Let's create a model using the penguin data,

```{r, echo = TRUE}
m2 <- lm(flipper_length_mm ~ bill_length_mm, data = data)
(c2 <- coefficients(m2))
```

- Let's create the predicted values,

```{r, echo = TRUE}
data <- data %>%
  mutate(yhat = c2[1] + c2[2]*bill_length_mm)
head(data, n = 2)
```

## Model Visualization: Example

- Now, let's build our `ggplot()`

::: {.panel-tabset}

## Code
```{r, echo = TRUE, eval = FALSE}
data %>% ggplot(aes(x = bill_length_mm, y = flipper_length_mm)) +
  geom_point(size=4, color = "darkgray") + 
  geom_line(aes(y = yhat)) +
  labs(x = "Bill Length (mm)",
       y = "Flipper Length (mm)") +
  theme_bw()
```

## Graph

<center>
```{r, echo = FALSE, eval = TRUE}
data %>% ggplot(aes(x = bill_length_mm, y = flipper_length_mm)) +
  geom_point(size=4, color = "darkgray") + 
  geom_line(aes(y = yhat)) +
  labs(x = "Bill Length (mm)",
       y = "Flipper Length (mm)") +
  theme_bw()
```
</center>

:::

## Multiple Regression: Introduction

- We have learned simple linear regression,

$$ \hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x $$

- Now, let's expand to multiple regression,

$$ \hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1  + \hat{\beta}_2 x_2 + ... + \hat{\beta}_k x_k$$

## Multiple Regression: Computation

- In simple linear regression, there are formulas for estimation of $\beta_0$ and $\beta_1$

- However, the true underlying computation is matrix multiplication: $$ y = X\beta + \varepsilon, $$ where:

    - $y$ is the vector of outcome responses,
    - $X$ is the design matrix
    
        - first column is all 1's
        - following columns are vectors of predictor responses
  
    - $\varepsilon$ is the vector of error terms
    
## Multiple Regression: R Syntax

- We again use the `lm()` function to construct our model.

```{r, echo = TRUE, eval = FALSE}
m <- lm([outcome] ~ [pred_1] + [pred_2] + ... + [pred_k], data = data)
```

- To see the full summary of the model, we will use the `summary()` function.
    
```{r, echo = TRUE, eval = FALSE}
summary(m)
``` 

- To find the confidence intervals for $\beta_i$, we will use the `confint()` function.

```{r, echo = TRUE, eval = FALSE}
confint(m)
```
    
## Multiple Regression: Example

- Recall the penguin data. Let's model body mass (g) as a function of bill length (mm) and flipper length (mm).

```{r, echo = TRUE}
data <- palmerpenguins::penguins
m1 <- lm(body_mass_g ~ bill_length_mm + flipper_length_mm, data = data)
summary(m1)
```

$$ \hat{y} = -5536.90 + 6.05 \text{ bill} + 48.15 \text{ flipper} $$
    
## Multiple Regression: Interpretations

- We interpret coefficients much the same as in simple linear regression.

    - Intercept: when [all predictors = 0], the average [outcome] is [$\hat{\beta}_0$].

    - Slope: For a 1 [units of predictor i] increase in [predictor i], we expect [outcome] to [increase or decrease] by [$|\hat{\beta}_i|$] [units of outcome], after controlling for [all other predictors in the model.
    
        - If $\hat{\beta}_i > 0$, there is an increase.
        - If $\hat{\beta}_i < 0$, there is a decrease.
    
## Multiple Regression: Example

- Let's interpret the penguin model.

```{r, echo = TRUE}
coefficients(m1)
```

- When both bill length and flipper length are 0 mm, a penguin's body mass is -5736.9 g.
- As bill length increases by 1 mm, the penguin's body mass increases by 6 g, after controlling for flipper length.
- As flipper length increases by 1 mm, the penguin's body mass increases by 48 g, after controlling for bill length.

## Multiple Regression: Inference on Slope

- **Hypotheses**

    - $H_0: \beta_i = \beta_i^{(0)}$
    - $H_1: \beta_i \ne \beta_i^{(0)}$

- **Test Statistic**

    - $$ t_0 = \frac{\hat{\beta}_i - \beta_i^{(0)}}{\text{SE}_{\hat{\beta}_i}}, $$ where

        -   $\beta_i^{(0)}$ is the hypothesized value (often 0)
        -   SE$_{\hat{\beta}_i}$ = the $i$^th^ diagonal entry of  $\sqrt{\text{MSE}(X^{'}X)^{-1}}$ 
        
- ***p*-Value**        

    - $p = 2 \times P[t_{1, 1 - \alpha/2} \ge t_0]$
    
- **Rejection Region**

    - Reject $H_0$ if $p < \alpha$
    
## Multiple Regression: R Syntax

- To see the *t*-test output, we will use the `summary()` function.
    
```{r, echo = TRUE, eval = FALSE}
summary(m)
``` 

- To find the confidence intervals for $\beta_i$, we will use the `confint()` function.

```{r, echo = TRUE, eval = FALSE}
confint(m)
```
    
## Multiple Regression: Example

```{r, echo = TRUE}
summary(m1)
```
    
## Multiple Regression: Example

- **Hypotheses**

    - $H_0: \beta_{\text{Bill}} = 0$
    - $H_1: \beta_{\text{Bill}} \ne 0$

- **Test Statistic and *p*-value**

    - $t_0 = 1.168$
    - $p = 0.244$
    
- **Rejection Region**

    - Reject $H_0$ if $p < \alpha$; $\alpha = 0.05$

- **Conclusion/Interpretation**

    - Fail to reject $H_0$. After adjusting for flipper length, there is not sufficient evidence to suggest that bill length is a significant predictor of penguin body mass.


## Multiple Regression: Example

- **Hypotheses**

    - $H_0: \beta_{\text{Flipper}} = 0$
    - $H_1: \beta_{\text{Flipper}} \ne 0$

- **Test Statistic and *p*-value**

    - $t_0 = 23.94$
    - $p < 0.001$
    
- **Rejection Region**

    - Reject $H_0$ if $p < \alpha$; $\alpha = 0.05$

- **Conclusion/Interpretation**

    - Reject $H_0$. After adjusting for bill length, there is sufficient evidence to suggest that flipper length is a significant predictor of penguin body mass.

## Multiple Regression: Example

```{r, echo = TRUE}
confint(m1)
```

- The 95% CI for $\beta_{\text{Bill}}$ is (-4.14, 16.24).

- The 95% CI for $\beta_{\text{Flipper}}$ is (44.19, 52.10).

## Multiple Regression: Inference on the Line

- ANOVA allows us to test for the significance of the regression line.

    - Partitioning the variance into variance due to the model and the error term.
    
    - The error term holds the variance due to variables *not* included in the predictor set.
    
- We will not concern ourselves with the computation of sums of squares.

    - Mean squares are computed like before: SS<sub>X</sub> / df<sub>X</sub>
    
    - Test statistic is *F* = MS<sub>Reg</sub> / MS<sub>E</sub>
    
    - The degrees of freedom are as follows:
    
        - df<sub>Reg</sub> = *k*
        - df<sub>E</sub> = *n*--*k*--1
        - df<sub>Tot</sub> = *n*--1
        
## Multiple Regression: Inference on the Line

- **Hypotheses**

    - $H_0: \beta_1 = \beta_2 = ... = \beta_k = 0$
    - $H_1:$ at least one $\beta_i$ is different.

- **Test Statistic**

    - $F_0 = \text{MS}_{\text{Reg}} / \text{MS}_{\text{E}}$ 

- ***p*-Value**

    - $p = P[F_{\text{df}_{\text{Reg}}, \text{df}_{\text{E}}, 1-\alpha/2} \ge F_0]$

- **Rejection Region**

    - Reject $H_0$ if $p < \alpha$; $\alpha = 0.05$

## Multiple Regression: R Syntax

- The ANOVA information comes out of the `summary()` function

```{r, echo = TRUE}
summary(m1)
```

## Multiple Regression: Example

- **Hypotheses**

    - $H_0: \beta_1 = \beta_2 = 0$
    - $H_1:$ at least one $\beta_i$ is different.

- **Test Statistic and *p*-Value**

    - $F_0 =  536.6$
    - $p < 0.001$
    
- **Rejection Region**

    - Reject $H_0$ if $p < \alpha$; $\alpha = 0.05$

- **Conclusion/Interpretation**

    - Reject $H_0$. There is sufficient evidence to suggest that at least one variable is a significant predictor of penguin body mass.

## Multiple Regression: Line Fit

- We can assess how well the regression model fits the data using $R^2$.

$$ R^2 = \frac{\text{SS}_{\text{Reg}}}{\text{SS}_{\text{Tot}}} $$

- Thus, $R^2$ is the proportion of variation explained by the model (i.e., predictor set).

- $R^2 \in [0, 1]$

    - $R^2 \to 0$ indicates that the model fits "poorly."
    
    - $R^2 \to 1$ indicates that the model fits "well."

    - $R^2 = 1$ indicates that all points fall on the response surface.
    


## Multiple Regression: Line Fit

- Recall that the error term is the "catch all" ...

    - The SS<sub>Tot</sub> is constant for the outcome of interest.
    - As we add predictors to the model, we are necessarily increasing SS<sub>Reg</sub>
    
        - The variance is moving from SS<sub>E</sub> to SS<sub>Reg</sub>
        
- We do not want to arbitrarily increase $R^2$, so we will use an adjusted version:

$$ R^2_{\text{adj}} = 1 - \frac{\text{MS}_{\text{E}}}{\text{SS}_{\text{Tot}}/\text{df}_{\text{Tot}}} $$

## Multiple Regression: R Syntax

- The $R^2$ and $R^2_{\text{adj}}$ both come out of the `summary()` function.

```{r, echo = TRUE}
summary(m1)
```

- $R^2_{\text{adj}}$ is 0.759 -- 75.9% of the variability in body mass is explained by the model with bill length and flipper length.

## Model Visualization: Introduction

- Like before, we can visualize our model. 

    - It is now more complicated because we have more than one predictor in the model.
    
- We will first plot a scatterplot, then overlay the regression line.

    - One predictor will be on the *x*-axis.
    - The outcome will be on the *y*-axis.
    - We can plot one or more lines by plugging in for the predictors not on the *x*-axis.
    
## Model Visualization: Example

- Let's create a model using the penguin data,

```{r, echo = TRUE}
(c1 <- coefficients(m1))
```    
    
- Let's have flipper length on the *x*-axis. This means we will plug in for bill length,

```{r, echo = TRUE}
quantile(data$bill_length_mm, na.rm = TRUE)
```

- Let's create lines for bill lengths of 35, 45, and 55 mm.

```{r, echo = TRUE}
library(tidyverse)
data <- data %>%
  mutate(b35 = c1[1] + c1[2]*35 + c1[3]*flipper_length_mm,
         b45 = c1[1] + c1[2]*45 + c1[3]*flipper_length_mm,
         b55 = c1[1] + c1[2]*55 + c1[3]*flipper_length_mm)
```    

## Model Visualization: Example

```{r}
head(data, n = 5)
```

## Model Visualization: Example

- Now we can construct the graph using ``ggplot()``:

::: {.panel-tabset}

## Code
```{r, echo = TRUE, eval = FALSE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "pink") +
  geom_line(aes(y = b45), color = "purple") +
  geom_line(aes(y = b55), color = "blue") +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```

## Graph
<center>
```{r, echo = FALSE, eval = TRUE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "pink") +
  geom_line(aes(y = b45), color = "purple") +
  geom_line(aes(y = b55), color = "blue") +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```
</center>

:::

## Model Visualization: Example

- We can further tweak the graph:

::: {.panel-tabset}

## Previous Code
```{r, echo = TRUE, eval = FALSE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "pink") +
  geom_line(aes(y = b45), color = "purple") +
  geom_line(aes(y = b55), color = "blue") +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```

## Previous Graph

<center>
```{r, echo = FALSE, eval = TRUE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "pink") +
  geom_line(aes(y = b45), color = "purple") +
  geom_line(aes(y = b55), color = "blue") +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```
</center>

## Updated Code

```{r, echo = TRUE, eval = FALSE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "pink") +
  geom_line(aes(y = b45), color = "purple") +
  geom_line(aes(y = b55), color = "blue") +
  geom_text(aes(x = 234, y = 5600, label = "35 mm")) +
  geom_text(aes(x = 234, y = 5700, label = "45 mm")) + 
  geom_text(aes(x = 234, y = 5800, label = "55 mm")) +
  geom_text(aes(x = 234, y = 5950, label = "Bill Length")) +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```

## Updated Graph

<center>
```{r, echo = FALSE, eval = TRUE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "pink") +
  geom_line(aes(y = b45), color = "purple") +
  geom_line(aes(y = b55), color = "blue") +
  geom_text(aes(x = 234, y = 5600, label = "35 mm")) +
  geom_text(aes(x = 234, y = 5700, label = "45 mm")) + 
  geom_text(aes(x = 234, y = 5800, label = "55 mm")) +
  geom_text(aes(x = 234, y = 5950, label = "Bill Length")) +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```
</center>

:::

## Model Visualization: Example

- We can further tweak the graph:

::: {.panel-tabset}

## Previous Code

```{r, echo = TRUE, eval = FALSE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "pink") +
  geom_line(aes(y = b45), color = "purple") +
  geom_line(aes(y = b55), color = "blue") +
  geom_text(aes(x = 234, y = 5600, label = "35 mm")) +
  geom_text(aes(x = 234, y = 5700, label = "45 mm")) + 
  geom_text(aes(x = 234, y = 5800, label = "55 mm")) +
  geom_text(aes(x = 234, y = 5950, label = "Bill Length")) +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```

## Previous Graph

<center>
```{r, echo = FALSE, eval = TRUE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "pink") +
  geom_line(aes(y = b45), color = "purple") +
  geom_line(aes(y = b55), color = "blue") +
  geom_text(aes(x = 234, y = 5600, label = "35 mm")) +
  geom_text(aes(x = 234, y = 5700, label = "45 mm")) + 
  geom_text(aes(x = 234, y = 5800, label = "55 mm")) +
  geom_text(aes(x = 234, y = 5950, label = "Bill Length")) +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```
</center>

## Updated Code

```{r, echo = TRUE, eval = FALSE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "black") +
  geom_line(aes(y = b45), color = "black") +
  geom_line(aes(y = b55), color = "black") +
  geom_text(aes(x = 234, y = 5600, label = "35 mm")) +
  geom_text(aes(x = 234, y = 5700, label = "45 mm")) + 
  geom_text(aes(x = 234, y = 5800, label = "55 mm")) +
  geom_text(aes(x = 234, y = 5950, label = "Bill Length")) +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```

## Updated Graph

<center>
```{r, echo = FALSE, eval = TRUE}
data %>% ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = b35), color = "black") +
  geom_line(aes(y = b45), color = "black") +
  geom_line(aes(y = b55), color = "black") +
  geom_text(aes(x = 234, y = 5600, label = "35 mm")) +
  geom_text(aes(x = 234, y = 5700, label = "45 mm")) + 
  geom_text(aes(x = 234, y = 5800, label = "55 mm")) +
  geom_text(aes(x = 234, y = 5950, label = "Bill Length")) +
  labs(x = "Flipper Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```
</center>

## Model Visualization: Example 2

- Let's create a model using the penguin data,

```{r, echo = TRUE}
c1 <- coefficients(m1)
c1
```    
    
- Let's have bill length on the *x*-axis. This means we will plug in for flipper length,

```{r, echo = TRUE}
quantile(data$flipper_length_mm, na.rm = TRUE)
```

- Let's create lines for bill lengths of 35, 45, and 55 mm.

```{r, echo = TRUE}
library(tidyverse)
data <- data %>%
  mutate(f175 = c1[1] + c1[2]*bill_length_mm + c1[3]*175,
         f200 = c1[1] + c1[2]*bill_length_mm + c1[3]*200,
         f225 = c1[1] + c1[2]*bill_length_mm + c1[3]*225)
``` 

## Model Visualization: Example 2

- Now we can construct the graph using ``ggplot()``:

::: {.panel-tabset}

## Code

```{r, echo = TRUE, eval = FALSE}
data %>% ggplot(aes(x = bill_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = f175), color = "pink") +
  geom_line(aes(y = f200), color = "purple") +
  geom_line(aes(y = f225), color = "blue") +
  labs(x = "Bill Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```

## Graph

<center>
```{r, echo = FALSE, eval = TRUE}
data %>% ggplot(aes(x = bill_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = f175), color = "pink") +
  geom_line(aes(y = f200), color = "purple") +
  geom_line(aes(y = f225), color = "blue") +
  labs(x = "Bill Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```
</center>

:::


## Model Visualization: Example 2

- After iterations of tweaking it, we have our final graph:

::: {.panel-tabset}

## Previous Code

```{r, echo = TRUE, eval = FALSE}
data %>% ggplot(aes(x = bill_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = f175), color = "pink") +
  geom_line(aes(y = f200), color = "purple") +
  geom_line(aes(y = f225), color = "blue") +
  labs(x = "Bill Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```

## Previous Graph

<center>
```{r, echo = FALSE, eval = TRUE}
data %>% ggplot(aes(x = bill_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = f175), color = "pink") +
  geom_line(aes(y = f200), color = "purple") +
  geom_line(aes(y = f225), color = "blue") +
  labs(x = "Bill Length (mm)",
       y = "Body Mass (g)") +
  theme_bw()
```
</center>

## Final Code

```{r, echo = TRUE, eval = FALSE}
data %>% ggplot(aes(x = bill_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = f175), color = "black") +
  geom_line(aes(y = f200), color = "black") +
  geom_line(aes(y = f225), color = "black") +
  labs(x = "Bill Length (mm)",
       y = "Body Mass (g)") +
  geom_text(aes(x = 65, y = 3010, label = "175 mm")) +
  geom_text(aes(x = 65, y = 4230, label = "200 mm")) +
  geom_text(aes(x = 65, y = 5480, label = "225 mm")) +
  geom_text(aes(x = 63, y = 5900, label = "Flipper Length")) +
  theme_bw()
```

## Final Graph

<center>
```{r, echo = FALSE, eval = TRUE}
data %>% ggplot(aes(x = bill_length_mm, y = body_mass_g)) +
  geom_point(size=3, color = "darkgray") + 
  geom_line(aes(y = f175), color = "black") +
  geom_line(aes(y = f200), color = "black") +
  geom_line(aes(y = f225), color = "black") +
  labs(x = "Bill Length (mm)",
       y = "Body Mass (g)") +
  geom_text(aes(x = 65, y = 3010, label = "175 mm")) +
  geom_text(aes(x = 65, y = 4230, label = "200 mm")) +
  geom_text(aes(x = 65, y = 5480, label = "225 mm")) +
  geom_text(aes(x = 63, y = 5900, label = "Flipper Length")) +
  theme_bw()
```
</center>

:::

## Wrap Up

- Today we have covered the basics of linear regression.

- When you move to other courses, you will learn more about linear regression - we have barely scratched the surface!

    - Categorical predictors.
    - Interaction terms.
    - Non-normal outcomes (categorical, count, skewed continuous).
    
- Remember that ANOVA = regression and regression = ANOVA!

    - We are just changing how we display the data/results.
    
    - Because of this, we already know how to assess the assumptions and was not discussed .

- Next week we will discuss categorical analysis.




